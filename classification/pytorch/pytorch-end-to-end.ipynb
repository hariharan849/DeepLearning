{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYgs_uhq6T2C"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "import kagglehub\n",
    "snmahsa_animal_image_dataset_cats_dogs_and_foxes_path = kagglehub.dataset_download('snmahsa/animal-image-dataset-cats-dogs-and-foxes')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-02T23:08:56.494455Z",
     "iopub.status.busy": "2024-12-02T23:08:56.493719Z",
     "iopub.status.idle": "2024-12-02T23:08:56.632128Z",
     "shell.execute_reply": "2024-12-02T23:08:56.631219Z",
     "shell.execute_reply.started": "2024-12-02T23:08:56.494416Z"
    },
    "id": "MxlrdcQ-6T2F",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:19:11.367547Z",
     "iopub.status.busy": "2024-12-02T23:19:11.36721Z",
     "iopub.status.idle": "2024-12-02T23:19:11.376273Z",
     "shell.execute_reply": "2024-12-02T23:19:11.375361Z",
     "shell.execute_reply.started": "2024-12-02T23:19:11.36752Z"
    },
    "id": "qYWSBdBN6T2H",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pathlib, cv2, os, copy, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from typing import Optional, Literal, Generator, Tuple, List, Dict\n",
    "from matplotlib.patches import Wedge\n",
    "from matplotlib.patheffects import withStroke\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "try:\n",
    "    import splitfolders\n",
    "except:\n",
    "    ! pip install split-folders\n",
    "    import splitfolders\n",
    "\n",
    "import torch, torchvision\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, ReLU, BatchNorm2d, Flatten, Linear, AvgPool2d, MaxPool2d, Dropout\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:08:56.644377Z",
     "iopub.status.busy": "2024-12-02T23:08:56.64406Z",
     "iopub.status.idle": "2024-12-02T23:08:56.659664Z",
     "shell.execute_reply": "2024-12-02T23:08:56.65882Z",
     "shell.execute_reply.started": "2024-12-02T23:08:56.644348Z"
    },
    "id": "nlDYRawh6T2I",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataframe_from_image_paths(image_folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\" Get images, labels and converts to dataframe\n",
    "\n",
    "        Args:\n",
    "            image_folder_path(str): Folders containing images\n",
    "    \"\"\"\n",
    "    images = list(pathlib.Path(image_folder_path).glob(\"*/*\"))\n",
    "    labels = [os.path.split(os.path.dirname(img_path))[-1] for img_path in images]\n",
    "    dataframe = pd.DataFrame(zip(images, labels), columns=[\"image_path\", \"labels\"])\n",
    "    dataframe[\"image_path\"] = dataframe[\"image_path\"].astype('str')\n",
    "    return dataframe\n",
    "\n",
    "def split_images_from_dataframe(dataframe: pd.DataFrame, val_split_size: int=0.3, test_split_size: int=0.1, split_test: bool=False)->tuple[list, list, list]:\n",
    "    \"\"\" Splits dataframe into train, validation and test(optional)\n",
    "\n",
    "        Args:\n",
    "            dataframe(pd.Dataframe): Dataset Dataframe to split\n",
    "            val_split_size(int): Validation split size\n",
    "            test_split_size(int): Test split size\n",
    "            split_test(bool): Flag to split validation further to test\n",
    "    \"\"\"\n",
    "    train, val = train_test_split(dataframe, test_size=val_split_size, stratify=dataframe[\"labels\"])\n",
    "    test = None\n",
    "    if split_test:\n",
    "        val, test = train_test_split(val, test_size=test_split_size)\n",
    "    return train, val, test\n",
    "\n",
    "def plot_class_distribution_in_pie_chart(dataframe: pd.DataFrame):\n",
    "    \"\"\" Plots dataset label distribution in pie chart\n",
    "\n",
    "        Args:\n",
    "            dataframe(pd.Dataframe): Dataset Dataframe to check class distribution\n",
    "    \"\"\"\n",
    "    # Assuming your data is in a pandas DataFrame called 'train_df'\n",
    "    animals_counts = dataframe['labels'].value_counts()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    wedges, texts, _ = ax.pie(\n",
    "        animals_counts.values.astype(\"float\"), startangle=90,\n",
    "        autopct='%1.1f%%', wedgeprops=dict(width=0.3, edgecolor='black')\n",
    "    )\n",
    "\n",
    "    # Add glow effect to each wedge\n",
    "    for wedge in wedges:\n",
    "        wedge.set_path_effects([withStroke(linewidth=6, foreground='cyan', alpha=0.4)])\n",
    "\n",
    "    # Customize chart labels\n",
    "    plt.legend(dataframe.index, loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1), fontsize=10, facecolor='#222222')\n",
    "\n",
    "    # Dark background for the cyberpunk look\n",
    "    fig.patch.set_facecolor('#2c2c2c')\n",
    "    ax.set_facecolor('#2c2c2c')\n",
    "\n",
    "    # Title\n",
    "    plt.title(\"Pie Chart\", color=\"white\", fontsize=16)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_class_distribution_in_count_chart(dataframe: pd.DataFrame):\n",
    "    \"\"\" Plots dataset label distribution in bar chart\n",
    "\n",
    "        Args:\n",
    "            dataframe(pd.Dataframe): Dataset Dataframe to check class distribution\n",
    "    \"\"\"\n",
    "    #count Plot\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.countplot(dataframe, x=\"labels\", palette='pastel')\n",
    "\n",
    "    # Annotate the count on top of each bar\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.annotate(f'{int(height)}',\n",
    "                    (p.get_x() + p.get_width() / 2, height),\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_random_images_per_class(dataframe: pd.DataFrame, no_images:int=2):\n",
    "    \"\"\" Plots images per label distribution\n",
    "\n",
    "        Args:\n",
    "            dataframe(pd.Dataframe): Dataset Dataframe to check class distribution\n",
    "    \"\"\"\n",
    "    class_names = dataframe[\"labels\"].unique()\n",
    "    random_images = dataframe.sample(frac=1).sort_values(by=\"labels\").groupby('labels').head(no_images)\n",
    "\n",
    "    count = 0\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    plt.figure(figsize=(12, num_classes * 4))\n",
    "\n",
    "    for index, (image_path, class_name) in random_images.iterrows():\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        count += 1\n",
    "        plt.subplot(num_classes, 2, count)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(class_name)\n",
    "\n",
    "def plot_percentage_split(train_dataset: np.array, val_dataset: np.array, test_dataset: Optional[np.array]=None):\n",
    "    \"\"\" Plots dataset split distribution in pie chart\n",
    "\n",
    "        Args:\n",
    "            train_dataset(np.array): Train dataset splitted\n",
    "            val_dataset(np.array): Validation dataset splitted\n",
    "        KwArgs:\n",
    "            test_dataset(np.array): Test dataset splitted\n",
    "    \"\"\"\n",
    "    train_size = len(train_dataset)\n",
    "    validation_size = len(val_dataset)\n",
    "    test_size = len(test_dataset or [])\n",
    "    # Dataset sizes\n",
    "    sizes = [train_size, validation_size]\n",
    "    labels = ['Train', 'Validation']\n",
    "    colors = ['#66c2a5', '#fc8d62']\n",
    "\n",
    "    if test_dataset:\n",
    "        sizes.append(test_size)\n",
    "        labels.append(\"Test\")\n",
    "        colors.append(\"#0000ff\")\n",
    "\n",
    "\n",
    "    def autopct_format(value):\n",
    "        \"\"\"Formats the autopct value to display the percentage and count.\"\"\"\n",
    "        total = sum(sizes)\n",
    "        percentage = f'{value:.1f}%'\n",
    "        count = int(value * total / 100)\n",
    "        return f'{percentage}\\n{count}'\n",
    "\n",
    "    # Create a pie chart\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct=autopct_format, startangle=140)\n",
    "    plt.title('Dataset Split Distribution', fontsize=16)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular.\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:08:56.679077Z",
     "iopub.status.busy": "2024-12-02T23:08:56.678768Z",
     "iopub.status.idle": "2024-12-02T23:08:56.689163Z",
     "shell.execute_reply": "2024-12-02T23:08:56.688399Z",
     "shell.execute_reply.started": "2024-12-02T23:08:56.679052Z"
    },
    "id": "RaLZjPug6T2J",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_torch_transforms():\n",
    "    \"\"\" Returns train and test transforms\n",
    "    \"\"\"\n",
    "    train_transforms = A.Compose(\n",
    "        transforms=[\n",
    "            A.Resize(height=224, width=224),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "            A.RandomCrop(height=128, width=128),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transform = A.Compose(\n",
    "        transforms=[\n",
    "            A.Resize(height=224, width=224),\n",
    "            A.CenterCrop(height=128, width=128),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return train_transforms, val_transform\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  \"\"\" Custom Dataset to apply transforms on dataframe\n",
    "\n",
    "  Args:\n",
    "    dataframe(pd.Dataframe): Dataset\n",
    "    transforms(list): List of transforms to apply\n",
    "  \"\"\"\n",
    "  def __init__(self, dataframe, transforms, class_to_idx):\n",
    "    super().__init__()\n",
    "    self._dataframe = dataframe\n",
    "    self._transforms = transforms\n",
    "    self._class_to_idx = class_to_idx\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\" Returns length of dataframe\n",
    "    \"\"\"\n",
    "    return len(self._dataframe)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\" Returns image and labels from specified index\n",
    "    \"\"\"\n",
    "    image_path, label = self._dataframe.iloc[index]\n",
    "    image = cv2.imread(image_path)\n",
    "    print(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transformed_image = self._transforms(image=image)[\"image\"]\n",
    "    return transformed_image, self._class_to_idx[label]\n",
    "\n",
    "def get_loaders(train_dataset, val_dataset, test_dataset, batch_size):\n",
    "    \"\"\" Returns Torch dataloader for train, val and test\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def visualize_augmentations(dataset, idx=0, samples=10, cols=5):\n",
    "    \"\"\" View the augmented images\n",
    "    \"\"\"\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset._transforms = A.Compose([t for t in dataset._transforms.transforms if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    for i in range(samples):\n",
    "        image, _ = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:08:56.691875Z",
     "iopub.status.busy": "2024-12-02T23:08:56.690878Z",
     "iopub.status.idle": "2024-12-02T23:08:56.70869Z",
     "shell.execute_reply": "2024-12-02T23:08:56.707976Z",
     "shell.execute_reply.started": "2024-12-02T23:08:56.691826Z"
    },
    "id": "CFajyHtp6T2K",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dataset creation to dataframe\n",
    "image_folder = os.path.join(\"/kaggle/input/animal-image-dataset-cats-dogs-and-foxes\", 'Animal Image Dataset-Cats, Dogs, and Foxes')\n",
    "dataframe = get_dataframe_from_image_paths(image_folder)\n",
    "class_names = dataframe[\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:08:56.709847Z",
     "iopub.status.busy": "2024-12-02T23:08:56.709609Z",
     "iopub.status.idle": "2024-12-02T23:09:12.086824Z",
     "shell.execute_reply": "2024-12-02T23:09:12.085886Z",
     "shell.execute_reply.started": "2024-12-02T23:08:56.709822Z"
    },
    "id": "J1t6dNVR6T2K",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# visualisation\n",
    "plot_class_distribution_in_pie_chart(dataframe)\n",
    "plot_class_distribution_in_count_chart(dataframe)\n",
    "plot_random_images_per_class(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:09:12.088465Z",
     "iopub.status.busy": "2024-12-02T23:09:12.0881Z",
     "iopub.status.idle": "2024-12-02T23:09:12.106681Z",
     "shell.execute_reply": "2024-12-02T23:09:12.105781Z",
     "shell.execute_reply.started": "2024-12-02T23:09:12.088428Z"
    },
    "id": "2KN8_hX76T2L",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_, val_, test_ = split_images_from_dataframe(dataframe, split_test=True)\n",
    "train_transforms, val_transforms = get_torch_transforms()\n",
    "\n",
    "class_to_idx = {cls_name: i for i, cls_name in enumerate(dataframe[\"labels\"].unique())}\n",
    "train_dataset = CustomDataset(train_, train_transforms, class_to_idx)\n",
    "val_dataset = CustomDataset(val_, val_transforms, class_to_idx)\n",
    "test_dataset = CustomDataset(test_, val_transforms, class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:09:12.110357Z",
     "iopub.status.busy": "2024-12-02T23:09:12.109662Z",
     "iopub.status.idle": "2024-12-02T23:09:16.829278Z",
     "shell.execute_reply": "2024-12-02T23:09:16.828369Z",
     "shell.execute_reply.started": "2024-12-02T23:09:12.110312Z"
    },
    "id": "qgFoSkCr6T2L",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_loaders(train_dataset, val_dataset, test_dataset, batch_size=32)\n",
    "visualize_augmentations(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:21:56.150618Z",
     "iopub.status.busy": "2024-12-02T23:21:56.150219Z",
     "iopub.status.idle": "2024-12-02T23:21:56.156758Z",
     "shell.execute_reply": "2024-12-02T23:21:56.15581Z",
     "shell.execute_reply.started": "2024-12-02T23:21:56.150586Z"
    },
    "id": "zKunUftC6T2M",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_writer(experiment_name: str,\n",
    "                  model_name: str,\n",
    "                  extra: str=None):\n",
    "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
    "\n",
    "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
    "\n",
    "    Where timestamp is the current date in YYYY-MM-DD format.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of experiment.\n",
    "        model_name (str): Name of model.\n",
    "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
    "\n",
    "    Example usage:\n",
    "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
    "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "                               model_name=\"effnetb2\",\n",
    "                               extra=\"5_epochs\")\n",
    "        # The above is the same as:\n",
    "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "    if extra:\n",
    "        # Create log directory path\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "\n",
    "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:20:41.98505Z",
     "iopub.status.busy": "2024-12-02T23:20:41.984211Z",
     "iopub.status.idle": "2024-12-02T23:20:42.756186Z",
     "shell.execute_reply": "2024-12-02T23:20:42.755284Z",
     "shell.execute_reply.started": "2024-12-02T23:20:41.985016Z"
    },
    "id": "_-t2rals6T2N",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "weights = torchvision.models.AlexNet_Weights.DEFAULT\n",
    "\n",
    "# Get the transforms used to create our pretrained weights\n",
    "auto_transforms = weights.transforms()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = torchvision.models.alexnet(weights=weights)\n",
    "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set the manual seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Get the length of class_names (one output unit for each class)\n",
    "output_shape = len(class_names)\n",
    "\n",
    "# Recreate the classifier layer and seed it to the target device\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True),\n",
    "    torch.nn.Linear(in_features=9216,\n",
    "                    out_features=512, # same number of output units as our number of classes\n",
    "                    bias=True),\n",
    "    torch.nn.Dropout(p=0.2, inplace=True),\n",
    "    torch.nn.Linear(in_features=512,\n",
    "                    out_features=output_shape, # same number of output units as our number of classes\n",
    "                    bias=True)).to(device)\n",
    "model = model.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=3)\n",
    "\n",
    "writer = create_writer(\n",
    "    experiment_name=\"Animal dataset\",\n",
    "    model_name=\"Alexnet\",\n",
    "    extra=\"learning animals dataset from torch api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:09:17.603195Z",
     "iopub.status.busy": "2024-12-02T23:09:17.602836Z",
     "iopub.status.idle": "2024-12-02T23:09:17.650358Z",
     "shell.execute_reply": "2024-12-02T23:09:17.649514Z",
     "shell.execute_reply.started": "2024-12-02T23:09:17.603155Z"
    },
    "id": "dnt0KZo96T2N",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "summary(model=model,\n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:09:17.652094Z",
     "iopub.status.busy": "2024-12-02T23:09:17.651713Z",
     "iopub.status.idle": "2024-12-02T23:09:17.666743Z",
     "shell.execute_reply": "2024-12-02T23:09:17.665861Z",
     "shell.execute_reply.started": "2024-12-02T23:09:17.652054Z"
    },
    "id": "3CarU-fT6T2O",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to training mode and then\n",
    "    runs through all of the required training steps (forward\n",
    "    pass, loss calculation, optimizer step).\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "    (0.1112, 0.8743)\n",
    "    \"\"\"\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        scheduler.step(loss.item())\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device,\n",
    "          writer: torch.utils.tensorboard.writer.SummaryWriter) -> Dict[str, List[float]]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "    writer: A SummaryWriter() instance to log model results to.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for\n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "              train_acc: [...],\n",
    "              test_loss: [...],\n",
    "              test_acc: [...]}\n",
    "    For example if training for epochs=2:\n",
    "             {train_loss: [2.0616, 1.0537],\n",
    "              train_acc: [0.3945, 0.3945],\n",
    "              test_loss: [1.2641, 1.5706],\n",
    "              test_acc: [0.3400, 0.2973]}\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        if writer:\n",
    "            # Add results to SummaryWriter\n",
    "            writer.add_scalars(main_tag=\"Loss\",\n",
    "                               tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                                \"test_loss\": test_loss},\n",
    "                               global_step=epoch)\n",
    "            writer.add_scalars(main_tag=\"Accuracy\",\n",
    "                               tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                                \"test_acc\": test_acc},\n",
    "                               global_step=epoch)\n",
    "\n",
    "            # Close the writer\n",
    "            writer.close()\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:09:17.668523Z",
     "iopub.status.busy": "2024-12-02T23:09:17.668067Z",
     "iopub.status.idle": "2024-12-02T23:12:47.485074Z",
     "shell.execute_reply": "2024-12-02T23:12:47.484136Z",
     "shell.execute_reply.started": "2024-12-02T23:09:17.668483Z"
    },
    "id": "F51UTR3-6T2P",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = train(\n",
    "    model, train_loader, test_loader, optimizer, loss_fn, 5, device, writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:12:47.486479Z",
     "iopub.status.busy": "2024-12-02T23:12:47.486104Z",
     "iopub.status.idle": "2024-12-02T23:12:47.495684Z",
     "shell.execute_reply": "2024-12-02T23:12:47.494711Z",
     "shell.execute_reply.started": "2024-12-02T23:12:47.486439Z"
    },
    "id": "6Y3SDlv66T2Q",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_loss_curves(results):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label=\"train_loss\")\n",
    "    plt.plot(epochs, test_loss, label=\"test_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
    "    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:12:47.498022Z",
     "iopub.status.busy": "2024-12-02T23:12:47.497072Z",
     "iopub.status.idle": "2024-12-02T23:12:47.996204Z",
     "shell.execute_reply": "2024-12-02T23:12:47.995366Z",
     "shell.execute_reply.started": "2024-12-02T23:12:47.497975Z"
    },
    "id": "6zCr1vA66T2Q",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:12:47.99775Z",
     "iopub.status.busy": "2024-12-02T23:12:47.997423Z",
     "iopub.status.idle": "2024-12-02T23:12:48.005871Z",
     "shell.execute_reply": "2024-12-02T23:12:48.004992Z",
     "shell.execute_reply.started": "2024-12-02T23:12:47.997718Z"
    },
    "id": "LCRkeav16T2R",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pred_and_plot_image(\n",
    "    model: torch.nn.Module,\n",
    "    class_names: List[str],\n",
    "    image_path: str,\n",
    "    target_image_act_label: str,\n",
    "    image_size: Tuple[int, int] = (224, 224),\n",
    "    transform: torchvision.transforms = None,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "    \"\"\"Predicts on a target image with a target model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A trained (or untrained) PyTorch model to predict on an image.\n",
    "        class_names (List[str]): A list of target classes to map predictions to.\n",
    "        image_path (str): Filepath to target image to predict on.\n",
    "        image_size (Tuple[int, int], optional): Size to transform target image to. Defaults to (224, 224).\n",
    "        transform (torchvision.transforms, optional): Transform to perform on image. Defaults to None which uses ImageNet normalization.\n",
    "        device (torch.device, optional): Target device to perform prediction on. Defaults to device.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create transformation for image (if one doesn't exist)\n",
    "    if transform is not None:\n",
    "        image_transform = transform\n",
    "    else:\n",
    "        image_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    ### Predict on image ###\n",
    "\n",
    "    # Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "        transformed_image = image_transform(image=img)[\"image\"].unsqueeze(dim=0)\n",
    "\n",
    "        # Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "    # Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "    # Plot image with predicted label and probability\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(\n",
    "        f\"Pred: {class_names[target_image_pred_label]} | Actual: {target_image_act_label} | Prob: {target_image_pred_probs.max():.3f}\"\n",
    "    )\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T23:12:48.0077Z",
     "iopub.status.busy": "2024-12-02T23:12:48.007009Z",
     "iopub.status.idle": "2024-12-02T23:12:50.012879Z",
     "shell.execute_reply": "2024-12-02T23:12:50.012031Z",
     "shell.execute_reply.started": "2024-12-02T23:12:48.007658Z"
    },
    "id": "7-9NjF6J6T2R",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_path, label = test_.iloc[random.randint(0, len(test_))]\n",
    "pred_and_plot_image(model, class_names, image_path, label, transform=val_transforms, device=device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "animals-alexnet",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6015941,
     "sourceId": 9812991,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
